1. without the self-attention block
Using :cpu
step 0: train loss 4.4801, val loss 4.4801
step 300: train loss 2.5404, val loss 2.5566
step 600: train loss 2.5160, val loss 2.5335
step 900: train loss 2.4967, val loss 2.5149
step 1200: train loss 2.5106, val loss 2.5254
step 1500: train loss 2.4853, val loss 2.5109
step 1800: train loss 2.4966, val loss 2.5198
step 2100: train loss 2.4949, val loss 2.5100
step 2400: train loss 2.4937, val loss 2.5102
step 2700: train loss 2.5040, val loss 2.5114


----- Using :cpu
step 0: train loss 4.2000, val loss 4.2047
step 500: train loss 2.6911, val loss 2.7087
step 1000: train loss 2.5196, val loss 2.5303
step 1500: train loss 2.4775, val loss 2.4829
step 2000: train loss 2.4408, val loss 2.4523
step 2500: train loss 2.4272, val loss 2.4435
step 3000: train loss 2.4130, val loss 2.4327
step 3500: train loss 2.3956, val loss 2.4212
step 4000: train loss 2.4041, val loss 2.3992
step 4500: train loss 2.3980, val loss 2.4084
--------

Wes le isen.
Woto teven INGO, ous into CYedd shou maithe ert thethens the the del ede cksy ow? Wlouby aicecat tisall wor
G'imemonou mar ee hacreancad hontrt had wousk ucavere.

Baraghe lfousto beme,
S m; ten gh;
S:
Ano ice de bay alysathef beatireplim serbeais I fard
Sy,
Me hallil:
DWAR: us,
Wte hse
--------

======================================================
            Multi-head self attention
======================================================
Using :cpu
step 0: train loss 4.2227, val loss 4.2226
step 500: train loss 2.6592, val loss 2.6733
step 1000: train loss 2.4980, val loss 2.5064
step 1500: train loss 2.4291, val loss 2.4349
step 2000: train loss 2.3716, val loss 2.3844
step 2500: train loss 2.3417, val loss 2.3561
step 3000: train loss 2.3149, val loss 2.3347
step 3500: train loss 2.2918, val loss 2.3171
step 4000: train loss 2.2895, val loss 2.2868
step 4500: train loss 2.2748, val loss 2.2858
--------

We! le ises.
Wmay they row we thutinte Caldd shou mait tiertlentthens the the dol ede cksy ba? Wlouby arceckentisste wre
G'imemonot mar ef hacr
COngd Go mringt thouskiu?

Fre.

Bardageplftisto be ess the to hon;
Soretr ice we bay, Thouthe wome isspe, laveberis I fald
Sy,
Whissitill there git; the se
--------


======================================================
            Feed-forward
======================================================
Using :cpu
step 0: train loss 4.1996, val loss 4.1995
step 500: train loss 2.5993, val loss 2.6077
step 1000: train loss 2.4629, val loss 2.4651
step 1500: train loss 2.3974, val loss 2.3951
step 2000: train loss 2.3297, val loss 2.3470
step 2500: train loss 2.3018, val loss 2.3221
step 3000: train loss 2.2828, val loss 2.2936
step 3500: train loss 2.2495, val loss 2.2721
step 4000: train loss 2.2435, val loss 2.2468
step 4500: train loss 2.2286, val loss 2.2411
--------

Ba hil thill shat coo he hot mes fin.

Cy hirad I four shat son yald hat lods guk- have ave lithr
GLOull
Wllld, with.

BANTAUCHAR:
I ork sak's willl eger aepale ganed ay wouce
song thy in noduace being uliths tot upiord--mard meme the fles,
Tharr, leanven-ty's thy on it in weancepte, digus I the sou
--------
======================================================
            After implementing the block
======================================================
Using :cpu
step 0: train loss 4.2116, val loss 4.2078
step 500: train loss 3.1045, val loss 3.1069
step 1000: train loss 2.7325, val loss 2.7215
step 1500: train loss 2.5818, val loss 2.5698
step 2000: train loss 2.4903, val loss 2.5037
step 2500: train loss 2.4491, val loss 2.4374
step 3000: train loss 2.3953, val loss 2.4151
step 3500: train loss 2.3749, val loss 2.3854
step 4000: train loss 2.3549, val loss 2.3395
step 4500: train loss 2.3152, val loss 2.3289
--------

thase he gatak an hay havis youf the lerdll what riveven:
I yow boncis ferge sate ceasace realverals you, thin indt do modes thoce and ose I! homome. DISNERN:
Thins thay srimes foorw.

MOIIS:
Tamle, rukee a, as Siod plodp han sutt, cofend yoo dot I:
I cent sete the the thongty mut vald sand yourd:
L
--------

Reason: this is not giving a good result because at this point we are starting to like a 
very deep NN and deep NNs suffer from optimization issues. So that's what we are slightly 
to run into. 

Now there are two ideas that we take from the paper
(refer to the Jupyter notebook)


======================================================
            After residual connection
======================================================
Using :cpu
step 0: train loss 4.6255, val loss 4.6233
step 500: train loss 2.3882, val loss 2.3850
step 1000: train loss 2.2704, val loss 2.2690
step 1500: train loss 2.1881, val loss 2.2103
step 2000: train loss 2.1456, val loss 2.1827
step 2500: train loss 2.1069, val loss 2.1541
step 3000: train loss 2.0695, val loss 2.1437
step 3500: train loss 2.0625, val loss 2.1203
step 4000: train loss 2.0281, val loss 2.1137
step 4500: train loss 2.0043, val loss 2.1024
--------


WIRICHARD RI:
Ay lad xobly of Eus, was ames
To do the adteeds prot am'd do coungs
Thereselss, yir should, wome Plove Gad brothen doward it thim the a will for, thy for-Mirkis dease, eit of I suels ardot, wiffell king daulywell I may we duse adied, seeving.
With's a singure what at your murderw:
Clo
--------
**OBSERVATIONS**
As you notice here the training loss is getting ahead of validation loss (in the graph form. which means tr loss is getting smaller)
so we started to see like a little bit of overfitting 

======================================================
            After Layer norm
======================================================
Using :cpu
step 0: train loss 4.3103, val loss 4.3097
step 500: train loss 2.4001, val loss 2.4010
step 1000: train loss 2.2644, val loss 2.2663
step 1500: train loss 2.1664, val loss 2.1896
step 2000: train loss 2.1321, val loss 2.1685
step 2500: train loss 2.0805, val loss 2.1298
step 3000: train loss 2.0506, val loss 2.1243
step 3500: train loss 2.0443, val loss 2.1045
step 4000: train loss 2.0143, val loss 2.0942
step 4500: train loss 1.9946, val loss 2.0988
--------


WIVIXANUS:
Tall catt xobly of know,
andames
To do to hath, of proolam'd rouch honow? EDWIUS:
Tyir should, wom, Pleev Gill but come hads it thight:
As will for, thy a greiars for quere thous of you mudot, wiff chake gread
ywall; man.

CORIOSAUS: Murdy him,
Asit,
You sile,
I what at your mus sow:
Cow
--------
**OBSERVATIONS**
- loss is down from 2.1024 to 2.0988. so slight improvement after adding the batch norm. 
- They will help more if we have bigger and deeper network 
======================================================
            scaled up model on Google Colab
======================================================

Using :cuda
step 0: train loss 4.2849, val loss 4.2823
step 500: train loss 2.0112, val loss 2.0971
step 1000: train loss 1.6021, val loss 1.7830
step 1500: train loss 1.4412, val loss 1.6396
step 2000: train loss 1.3430, val loss 1.5724
step 2500: train loss 1.2809, val loss 1.5330
step 3000: train loss 1.2268, val loss 1.5094
step 3500: train loss 1.1824, val loss 1.4881
step 4000: train loss 1.1475, val loss 1.4869
step 4500: train loss 1.1108, val loss 1.4805
--------

But with prison: I will stead with you.

ISABELLA:
Cars serve you thank for's ar a wife:
And see thou wast to Harle, no:
Ah, have what you speake upon your sister,
Than your honour ancess, so sovereign along.

LADY CLIFF:
How shouldst thou lie and soonest in heapted,
That you stead'st a mystery to f